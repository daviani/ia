model_list:
  - model_name: qwen3-dev
    litellm_params:
      model: ollama/qwen3-coder:30b
      api_base: http://127.0.0.1:11434
      temperature: 0.2
      top_p: 0.9
      num_ctx: 131072
      stream: true

litellm_settings:
  drop_params: true        # ignore paramètres OpenAI non supportés par Ollama
  set_verbose: false

router_settings:
  timeout: 600

general_settings:
  master_key: dummy-key    # clef acceptée par le proxy (libre, non sensible)
  telemetry: false
